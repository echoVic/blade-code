# CLAUDE.md

always response in Chinese

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## ğŸ“¦ åŒ…ç®¡ç†å·¥å…·

é¡¹ç›®ä½¿ç”¨ **pnpm** ä½œä¸ºåŒ…ç®¡ç†å·¥å…·ï¼Œæ”¯æŒ Node.js 16.0.0 åŠä»¥ä¸Šç‰ˆæœ¬ã€‚

```bash
# å®‰è£…ä¾èµ–
pnpm install

# å…¨å±€å®‰è£… Blade CLI
pnpm add -g blade-ai

# æœ¬åœ°å¼€å‘å®‰è£…
pnpm link
```

## ğŸ› ï¸ æŠ€æœ¯æ ˆ

### æ ¸å¿ƒæŠ€æœ¯
- **TypeScript 5.9+**: ç±»å‹å®‰å…¨çš„ JavaScript è¶…é›†
- **Node.js 16+**: JavaScript è¿è¡Œæ—¶ç¯å¢ƒ
- **tsup**: å¿«é€Ÿ TypeScript æ„å»ºå·¥å…·
- **ESM**: ES æ¨¡å—ç³»ç»Ÿ

### CLI æ¡†æ¶
- **Commander.js**: å‘½ä»¤è¡Œæ¥å£æ¡†æ¶
- **Inquirer.js**: äº¤äº’å¼å‘½ä»¤è¡Œç”¨æˆ·ç•Œé¢
- **Chalk**: ç»ˆç«¯å­—ç¬¦ä¸²æ ·å¼åº“
- **Ink + React**: åŸºäº React çš„ CLI åº”ç”¨æ„å»º

### AI é›†æˆ
- **OpenAI SDK**: OpenAI API é›†æˆ
- **Model Context Protocol (MCP)**: AI æ¨¡å‹ä¸Šä¸‹æ–‡åè®®
- **WebSocket**: å®æ—¶é€šä¿¡æ”¯æŒ

### å¼€å‘å·¥å…·
- **ESLint**: ä»£ç è´¨é‡æ£€æŸ¥
- **Prettier**: ä»£ç æ ¼å¼åŒ–
- **TypeScript Compiler**: ç±»å‹æ£€æŸ¥
- **ts-node**: TypeScript ç›´æ¥æ‰§è¡Œ

### ç½‘ç»œè¯·æ±‚
- **Axios**: HTTP å®¢æˆ·ç«¯åº“

## ğŸ”§ Development Commands

Core commands for building and maintaining this TypeScript CLI tool:

```bash
# Development
pnpm run dev          # Watch mode development build
pnpm run build        # Production build with tsup
pnpm run start        # Run the CLI directly: node bin/blade.js

# Code Quality
pnpm run type-check   # TypeScript compilation check
pnpm run lint         # ESLint for code quality
pnpm run format       # Prettier for code formatting
pnpm run format:check # Check formatting without fixing
pnpm run check        # Run all quality checks combined

# Release management
pnpm run release              # Create new release
pnpm run release:dry          # Dry run release process
pnpm run release:patch        # Patch version bump
pnpm run release:minor        # Minor version bump
pnpm run release:major        # Major version bump
```

## ğŸ—ï¸ Architecture Overview

Blade is an **AI-first CLI agent** built with a component-based architecture around a central Agent class that **embeds LLM capabilities directly** rather than treating them as external components.

### Core Design Philosophy

```
Agent = LLMs + System Prompt + Context + Tools
```

This architecture allows the Agent to be the single entry point that houses AI capabilities internally, making LLM usage feel native rather than like an external service.

### Key Architecture Patterns

**1. Agent-as-Entry-Point Pattern**
- Agent class centrally coordinates LLM, tools, context management
- LLM providers (Qwen, VolcEngine) are **embedded** not external
- Components register with the Agent rather than the other way around

**2. Component-Based System**
- **LLMManager**: Houses QwenLLM and VolcEngineLLM implementations
- **ToolComponent**: Manages 25+ builtin tools (Git, file system, utilities)
- **ContextComponent**: Conversation memory and context management
- **MCPComponent**: Model Context Protocol integration
- **LoggerComponent**: Centralized logging

**3. Modular Tooling**
- Tools are organized by category (git-tools, filesystem, networking)
- Smart tools powered by LLM (code review, documentation generation)
- Security confirmation system for dangerous operations

### Key Directories & Patterns

```
src/
â”œâ”€â”€ agent/            # Core Agent class and component system
â”‚   â”œâ”€â”€ Agent.ts      # Unified agent with embedded LLM
â”‚   â”œâ”€â”€ LLMManager.ts # Internal LLM orchestration
â”‚   â””â”€â”€ *.ts          # Component implementations
â”œâ”€â”€ llm/              # LLM provider implementations
â”‚   â”œâ”€â”€ BaseLLM.ts    # Abstract LLM interface
â”‚   â”œâ”€â”€ QwenLLM.ts    # Alibaba Cloud Qwen integration
â”‚   â””â”€â”€ VolcEngineLLM.ts # VolcEngine/Doubao integration
â”œâ”€â”€ tools/            # 25+ builtin tools
â”‚   â”œâ”€â”€ builtin/      # Git tools, filesystem, networking
â”‚   â””â”€â”€ smart-tools/  # LLM-powered intelligent tools
â”œâ”€â”€ context/          # Conversation memory and context
â””â”€â”€ commands/         # CLI entry points
```

### Development Workflow

1. **Create agent**: `new Agent({ llm: { provider: 'qwen', apiKey } })`
2. **Initialize**: `await agent.init()` - sets up LLM + components
3. **Use capabilities**: Direct methods like `agent.chat()`, `agent.generateCode()`
4. **Cleanup**: `await agent.destroy()` - proper lifecycle management

### Testing Patterns

Tests focus on validating Agent behavior rather than individual LLM calls:
- Component initialization and cleanup
- Tool registration and security confirmations
- Context memory and conversation flow
- CLI command outputs

The architecture emphasizes **AI-native CLI design** where the agent is the intelligent interface, not just a wrapper around external AI services.