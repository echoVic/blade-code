import OpenAI from 'openai';
import type {
  ChatCompletionMessageParam,
  ChatCompletionMessageToolCall,
  ChatCompletionTool,
} from 'openai/resources/chat';
import { createLogger, LogCategory } from '../logging/Logger.js';
import type {
  ChatConfig,
  ChatResponse,
  IChatService,
  Message,
  ReasoningFieldName,
  StreamChunk,
} from './ChatServiceInterface.js';

const _logger = createLogger(LogCategory.CHAT);

/**
 * æ”¯æŒçš„ reasoning å­—æ®µååˆ—è¡¨ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
 * ä¸åŒ API ä»£ç†ä½¿ç”¨ä¸åŒçš„å­—æ®µåï¼š
 * - reasoning_content: DeepSeek å®˜æ–¹ API
 * - reasoning: zenmux.ai ç­‰ä»£ç†
 * - reasoningContent: æŸäº›ä»£ç†ä½¿ç”¨é©¼å³°å‘½å
 * - thinking_content: æŸäº›ä»£ç†ä½¿ç”¨è¿™ä¸ªåç§°
 */
const REASONING_FIELD_NAMES: ReasoningFieldName[] = [
  'reasoning_content',
  'reasoning',
  'reasoningContent',
  'thinking_content',
];

/**
 * å¸¦ reasoning å­—æ®µçš„æ‰©å±•æ¶ˆæ¯ç±»å‹
 */
type ExtendedMessageWithReasoning = {
  reasoning_content?: string;
  reasoning?: string;
  reasoningContent?: string;
  thinking_content?: string;
};

/**
 * ä»æ‰©å±•æ¶ˆæ¯ä¸­æå– reasoning å†…å®¹
 * @returns { content, fieldName } æˆ– undefined
 */
function extractReasoningFromMessage(
  message: ExtendedMessageWithReasoning
): { content: string; fieldName: ReasoningFieldName } | undefined {
  for (const fieldName of REASONING_FIELD_NAMES) {
    const value = message[fieldName];
    if (value) {
      return { content: value, fieldName };
    }
  }
  return undefined;
}

/**
 * è¿‡æ»¤å­¤å„¿ tool æ¶ˆæ¯
 *
 * å­¤å„¿ tool æ¶ˆæ¯æ˜¯æŒ‡ tool_call_id å¯¹åº”çš„ assistant æ¶ˆæ¯ä¸å­˜åœ¨çš„ tool æ¶ˆæ¯ã€‚
 * è¿™ç§æƒ…å†µé€šå¸¸å‘ç”Ÿåœ¨ä¸Šä¸‹æ–‡å‹ç¼©åï¼Œå¯¼è‡´ OpenAI API è¿”å› 400 é”™è¯¯ã€‚
 */
function filterOrphanToolMessages(messages: Message[]): Message[] {
  // æ”¶é›†æ‰€æœ‰å¯ç”¨çš„ tool_call ID
  const availableToolCallIds = new Set<string>();
  for (const msg of messages) {
    if (msg.role === 'assistant' && msg.tool_calls) {
      for (const tc of msg.tool_calls) {
        availableToolCallIds.add(tc.id);
      }
    }
  }

  // è¿‡æ»¤æ‰å­¤å„¿ tool æ¶ˆæ¯
  return messages.filter((msg) => {
    if (msg.role === 'tool') {
      // ç¼ºå¤± tool_call_id çš„ tool æ¶ˆæ¯ç›´æ¥ä¸¢å¼ƒ
      if (!msg.tool_call_id) {
        return false;
      }
      return availableToolCallIds.has(msg.tool_call_id);
    }
    return true;
  });
}

export class OpenAIChatService implements IChatService {
  private client: OpenAI;
  // è‡ªåŠ¨æ£€æµ‹åˆ°çš„ reasoning å­—æ®µåï¼ˆAPI ä»£ç†å¯èƒ½ä½¿ç”¨ä¸åŒçš„å­—æ®µåï¼‰
  private detectedReasoningFieldName: ReasoningFieldName | null = null;

  /**
   * å°†å†…éƒ¨ Message è½¬æ¢ä¸º OpenAI API æ ¼å¼
   * ç»Ÿä¸€å¤„ç† tool æ¶ˆæ¯ã€assistant æ¶ˆæ¯ï¼ˆå« tool_callsï¼‰ã€æ™®é€šæ¶ˆæ¯
   * æ”¯æŒå¤šæ¨¡æ€å†…å®¹ï¼ˆæ–‡æœ¬ + å›¾ç‰‡ï¼‰
   */
  private convertToOpenAIMessages(messages: Message[]): ChatCompletionMessageParam[] {
    return messages.map((msg) => {
      if (msg.role === 'tool') {
        // tool æ¶ˆæ¯çš„ content å§‹ç»ˆæ˜¯å­—ç¬¦ä¸²
        const toolContent = typeof msg.content === 'string'
          ? msg.content
          : msg.content.filter((p) => p.type === 'text').map((p) => p.text).join('\n');
        return {
          role: 'tool',
          content: toolContent,
          tool_call_id: msg.tool_call_id!,
        };
      }

      if (msg.role === 'assistant' && msg.tool_calls) {
        // assistant æ¶ˆæ¯çš„ content å§‹ç»ˆæ˜¯å­—ç¬¦ä¸²æˆ– null
        const assistantContent = typeof msg.content === 'string'
          ? msg.content
          : msg.content.filter((p) => p.type === 'text').map((p) => p.text).join('\n');

        const baseMessage: any = {
          role: 'assistant',
          content: assistantContent || null,
          tool_calls: msg.tool_calls,
        };

        // åªæœ‰ thinking æ¨¡å‹æ‰éœ€è¦ reasoning å­—æ®µ
        // DeepSeek API è¦æ±‚ï¼šåŒ…å« tool_calls çš„ assistant æ¶ˆæ¯å¿…é¡»æœ‰ reasoning å­—æ®µ
        // å‚è€ƒï¼šhttps://api-docs.deepseek.com/guides/thinking_mode#tool-calls
        if (this.config.supportsThinking) {
          const reasoningValue =
            'reasoningContent' in msg && msg.reasoningContent ? msg.reasoningContent : '';

          if (this.detectedReasoningFieldName) {
            // å·²æ£€æµ‹åˆ°å­—æ®µåï¼Œä½¿ç”¨æ£€æµ‹åˆ°çš„å­—æ®µå
            baseMessage[this.detectedReasoningFieldName] = reasoningValue;
          } else {
            // æœªæ£€æµ‹åˆ°å­—æ®µåï¼ŒåŒæ—¶å‘é€æ‰€æœ‰å¯èƒ½çš„å­—æ®µåï¼Œé¿å…æ­»é”
            // è¿™æ ·æ— è®ºä»£ç†æ¥å—å“ªä¸ªå­—æ®µåéƒ½èƒ½æ­£å¸¸å·¥ä½œ
            for (const fieldName of REASONING_FIELD_NAMES) {
              baseMessage[fieldName] = reasoningValue;
            }
          }
        }

        return baseMessage;
      }

      // å¤„ç† user/system/assistant æ¶ˆæ¯
      // user æ¶ˆæ¯å¯èƒ½åŒ…å«å¤šæ¨¡æ€å†…å®¹ï¼ˆæ–‡æœ¬ + å›¾ç‰‡ï¼‰
      if (msg.role === 'user' && Array.isArray(msg.content)) {
        // å¤šæ¨¡æ€ user æ¶ˆæ¯ï¼šè½¬æ¢ä¸º OpenAI Vision API æ ¼å¼
        return {
          role: 'user',
          content: msg.content.map((part) => {
            if (part.type === 'text') {
              return { type: 'text' as const, text: part.text };
            }
            // image_url ç±»å‹
            return {
              type: 'image_url' as const,
              image_url: { url: part.image_url.url },
            };
          }),
        };
      }

      // æ™®é€šæ–‡æœ¬æ¶ˆæ¯
      return {
        role: msg.role as 'user' | 'assistant' | 'system',
        content: typeof msg.content === 'string'
          ? msg.content
          : msg.content.filter((p) => p.type === 'text').map((p) => p.text).join('\n'),
      };
    });
  }

  /**
   * å°†å·¥å…·å®šä¹‰è½¬æ¢ä¸º OpenAI API æ ¼å¼
   */
  private convertToOpenAITools(
    tools?: Array<{ name: string; description: string; parameters: any }>
  ): ChatCompletionTool[] | undefined {
    return tools?.map((tool) => ({
      type: 'function' as const,
      function: {
        name: tool.name,
        description: tool.description,
        parameters: tool.parameters,
      },
    }));
  }

  /**
   * ä» API å“åº”ä¸­æå– reasoning å¹¶æ›´æ–°æ£€æµ‹åˆ°çš„å­—æ®µå
   */
  private extractAndDetectReasoning(message: ExtendedMessageWithReasoning): string | undefined {
    const result = extractReasoningFromMessage(message);
    if (result) {
      // ä¿å­˜æ£€æµ‹åˆ°çš„å­—æ®µåï¼ˆç”¨äºåç»­å‘é€è¯·æ±‚æ—¶ä½¿ç”¨ç›¸åŒçš„å­—æ®µåï¼‰
      if (!this.detectedReasoningFieldName) {
        this.detectedReasoningFieldName = result.fieldName;
        _logger.debug(`ğŸ§  [ChatService] Detected reasoning field: ${result.fieldName}`);
      }
      return result.content;
    }
    return undefined;
  }

  constructor(private config: ChatConfig) {
    _logger.debug('ğŸš€ [ChatService] Initializing ChatService');
    _logger.debug('âš™ï¸ [ChatService] Config:', {
      model: config.model,
      baseUrl: config.baseUrl,
      temperature: config.temperature,
      maxContextTokens: config.maxContextTokens,
      timeout: config.timeout,
      hasApiKey: !!config.apiKey,
    });

    if (!config.baseUrl) {
      _logger.error('âŒ [ChatService] baseUrl is required in ChatConfig');
      throw new Error('baseUrl is required in ChatConfig');
    }
    if (!config.apiKey) {
      _logger.error('âŒ [ChatService] apiKey is required in ChatConfig');
      throw new Error('apiKey is required in ChatConfig');
    }
    if (!config.model) {
      _logger.error('âŒ [ChatService] model is required in ChatConfig');
      throw new Error('model is required in ChatConfig');
    }

    this.client = new OpenAI({
      apiKey: config.apiKey,
      baseURL: config.baseUrl, // OpenAI SDK ä½¿ç”¨ baseURL
      timeout: config.timeout ?? 180000, // 180ç§’è¶…æ—¶ï¼ˆé•¿ä¸Šä¸‹æ–‡åœºæ™¯éœ€è¦æ›´é•¿æ—¶é—´ï¼‰
      maxRetries: 3,
    });

    _logger.debug('âœ… [ChatService] ChatService initialized successfully');
  }

  async chat(
    messages: Message[],
    tools?: Array<{
      name: string;
      description: string;
      parameters: any;
    }>,
    signal?: AbortSignal
  ): Promise<ChatResponse> {
    const startTime = Date.now();
    _logger.debug('ğŸš€ [ChatService] Starting chat request');
    _logger.debug('ğŸ“ [ChatService] Messages count:', messages.length);

    // è¿‡æ»¤å­¤å„¿ tool æ¶ˆæ¯
    const filteredMessages = filterOrphanToolMessages(messages);
    if (filteredMessages.length < messages.length) {
      _logger.debug(
        `ğŸ”§ [ChatService] è¿‡æ»¤æ‰ ${messages.length - filteredMessages.length} æ¡å­¤å„¿ tool æ¶ˆæ¯`
      );
    }

    _logger.debug(
      'ğŸ“ [ChatService] Messages preview:',
      filteredMessages.map((m) => ({
        role: m.role,
        contentLength: typeof m.content === 'string' ? m.content.length : m.content.length,
        isMultimodal: Array.isArray(m.content),
      }))
    );

    const openaiMessages = this.convertToOpenAIMessages(filteredMessages);
    const openaiTools = this.convertToOpenAITools(tools);

    _logger.debug('ğŸ”§ [ChatService] Tools count:', openaiTools?.length || 0);
    if (openaiTools && openaiTools.length > 0) {
      _logger.debug(
        'ğŸ”§ [ChatService] Available tools:',
        openaiTools.map((t) => (t.type === 'function' ? t.function.name : 'unknown'))
      );
    }

    const requestParams = {
      model: this.config.model,
      messages: openaiMessages,
      tools: openaiTools,
      tool_choice:
        openaiTools && openaiTools.length > 0 ? ('auto' as const) : undefined,
      max_tokens: this.config.maxOutputTokens ?? 32768,
      temperature: this.config.temperature ?? 0.0,
    };

    _logger.debug('ğŸ“¤ [ChatService] Request params:', {
      model: requestParams.model,
      messagesCount: requestParams.messages.length,
      toolsCount: requestParams.tools?.length || 0,
      tool_choice: requestParams.tool_choice,
      max_tokens: requestParams.max_tokens,
      temperature: requestParams.temperature,
    });

    try {
      const completion = await this.client.chat.completions.create(requestParams, {
        signal,
      });
      const requestDuration = Date.now() - startTime;

      _logger.debug('ğŸ“¥ [ChatService] Response received in', requestDuration, 'ms');

      // âœ… éªŒè¯å“åº”æ ¼å¼
      if (!completion) {
        _logger.error('âŒ [ChatService] API returned null/undefined response');
        throw new Error('API returned null/undefined response');
      }

      if (!completion.choices || !Array.isArray(completion.choices)) {
        _logger.error(
          'âŒ [ChatService] Invalid API response format - missing choices array'
        );
        _logger.error(
          'âŒ [ChatService] Response object:',
          JSON.stringify(completion, null, 2)
        );
        throw new Error(
          `Invalid API response: missing choices array. Response: ${JSON.stringify(completion)}`
        );
      }

      if (completion.choices.length === 0) {
        _logger.error('âŒ [ChatService] API returned empty choices array');
        throw new Error('API returned empty choices array');
      }

      _logger.debug('ğŸ“Š [ChatService] Response usage:', completion.usage);
      _logger.debug(
        'ğŸ“Š [ChatService] Response choices count:',
        completion.choices.length
      );

      const choice = completion.choices[0];
      if (!choice) {
        _logger.error('âŒ [ChatService] No completion choice returned');
        throw new Error('No completion choice returned');
      }

      _logger.debug('ğŸ“ [ChatService] Response choice:', {
        finishReason: choice.finish_reason,
        contentLength: choice.message.content?.length || 0,
        hasToolCalls: !!choice.message.tool_calls,
        toolCallsCount: choice.message.tool_calls?.length || 0,
      });

      if (choice.message.tool_calls) {
        _logger.debug(
          'ğŸ”§ [ChatService] Tool calls:',
          choice.message.tool_calls.map((tc) => ({
            id: tc.id,
            type: tc.type,
            functionName: tc.type === 'function' ? tc.function?.name : 'unknown',
            functionArgsLength:
              tc.type === 'function' ? tc.function?.arguments?.length || 0 : 0,
          }))
        );
      }

      const toolCalls = choice.message.tool_calls?.filter(
        (tc): tc is ChatCompletionMessageToolCall => tc.type === 'function'
      );

      // æå– reasoningï¼ˆDeepSeek R1 ç­‰ thinking æ¨¡å‹çš„æ‰©å±•å­—æ®µï¼‰
      const extendedMessage = choice.message as typeof choice.message & ExtendedMessageWithReasoning;
      const reasoningContent = this.extractAndDetectReasoning(extendedMessage);

      // è°ƒè¯•æ—¥å¿—ï¼šæ£€æŸ¥ API å®é™…è¿”å›çš„å­—æ®µ
      if (this.config.supportsThinking) {
        _logger.debug('ğŸ§  [ChatService] Thinking model response:', {
          reasoningContentLength: reasoningContent?.length || 0,
          detectedFieldName: this.detectedReasoningFieldName,
          messageKeys: Object.keys(choice.message),
        });
      }

      // æå– reasoning_tokensï¼ˆthinking æ¨¡å‹çš„æ‰©å±• usage å­—æ®µï¼‰
      const extendedUsage = completion.usage as typeof completion.usage & {
        reasoning_tokens?: number;
      };

      const response = {
        content: choice.message.content || '',
        reasoningContent,
        toolCalls: toolCalls,
        usage: {
          promptTokens: completion.usage?.prompt_tokens || 0,
          completionTokens: completion.usage?.completion_tokens || 0,
          totalTokens: completion.usage?.total_tokens || 0,
          reasoningTokens: extendedUsage?.reasoning_tokens,
        },
      };

      _logger.debug('âœ… [ChatService] Chat completed successfully');
      _logger.debug('ğŸ“Š [ChatService] Final response:', {
        contentLength: response.content.length,
        toolCallsCount: response.toolCalls?.length || 0,
        usage: response.usage,
      });

      return response;
    } catch (error) {
      const requestDuration = Date.now() - startTime;
      _logger.error(
        'âŒ [ChatService] Chat request failed after',
        requestDuration,
        'ms'
      );
      _logger.error('âŒ [ChatService] Error details:', error);
      throw error;
    }
  }

  async *streamChat(
    messages: Message[],
    tools?: Array<{
      name: string;
      description: string;
      // biome-ignore lint/suspicious/noExplicitAny: å·¥å…·å‚æ•°æ ¼å¼ä¸ç¡®å®š
      parameters: any;
    }>,
    signal?: AbortSignal
  ): AsyncGenerator<StreamChunk, void, unknown> {
    const startTime = Date.now();
    _logger.debug('ğŸš€ [ChatService] Starting chat stream request');
    _logger.debug('ğŸ“ [ChatService] Messages count:', messages.length);

    // è¿‡æ»¤å­¤å„¿ tool æ¶ˆæ¯
    const filteredMessages = filterOrphanToolMessages(messages);
    if (filteredMessages.length < messages.length) {
      _logger.debug(
        `ğŸ”§ [ChatService] è¿‡æ»¤æ‰ ${messages.length - filteredMessages.length} æ¡å­¤å„¿ tool æ¶ˆæ¯`
      );
    }

    _logger.debug(
      'ğŸ“ [ChatService] Messages preview:',
      filteredMessages.map((m) => ({
        role: m.role,
        contentLength: typeof m.content === 'string' ? m.content.length : m.content.length,
        isMultimodal: Array.isArray(m.content),
      }))
    );

    const openaiMessages = this.convertToOpenAIMessages(filteredMessages);
    const openaiTools = this.convertToOpenAITools(tools);

    _logger.debug('ğŸ”§ [ChatService] Stream tools count:', openaiTools?.length || 0);
    if (openaiTools && openaiTools.length > 0) {
      _logger.debug(
        'ğŸ”§ [ChatService] Stream available tools:',
        openaiTools.map((t) => (t.type === 'function' ? t.function.name : 'unknown'))
      );
    }

    const requestParams = {
      model: this.config.model,
      messages: openaiMessages,
      tools: openaiTools,
      tool_choice:
        openaiTools && openaiTools.length > 0 ? ('auto' as const) : ('none' as const),
      max_tokens: this.config.maxOutputTokens ?? 32768,
      temperature: this.config.temperature ?? 0.0,
      stream: true as const,
    };

    _logger.debug('ğŸ“¤ [ChatService] Stream request params:', {
      model: requestParams.model,
      messagesCount: requestParams.messages.length,
      toolsCount: requestParams.tools?.length || 0,
      tool_choice: requestParams.tool_choice,
      max_tokens: requestParams.max_tokens,
      temperature: requestParams.temperature,
      stream: requestParams.stream,
    });

    try {
      const stream = await this.client.chat.completions.create(requestParams, {
        signal,
      });
      const requestDuration = Date.now() - startTime;
      _logger.debug('ğŸ“¥ [ChatService] Stream started in', requestDuration, 'ms');

      let chunkCount = 0;
      let totalContent = '';
      let totalReasoningContent = '';
      let toolCallsReceived = false;

      for await (const chunk of stream) {
        chunkCount++;

        // âœ… éªŒè¯ chunk æ ¼å¼
        if (!chunk || !chunk.choices || !Array.isArray(chunk.choices)) {
          _logger.warn('âš ï¸ [ChatService] Invalid chunk format in stream', chunkCount);
          continue;
        }

        const delta = chunk.choices[0]?.delta;
        if (!delta) {
          _logger.warn('âš ï¸ [ChatService] Empty delta in chunk', chunkCount);
          continue;
        }

        // æå– reasoningï¼ˆDeepSeek R1 ç­‰ thinking æ¨¡å‹çš„æ‰©å±•å­—æ®µï¼‰
        const extendedDelta = delta as typeof delta & ExtendedMessageWithReasoning;
        const reasoningChunk = this.extractAndDetectReasoning(extendedDelta);

        if (delta.content) {
          totalContent += delta.content;
        }

        if (reasoningChunk) {
          totalReasoningContent += reasoningChunk;
        }

        if (delta.tool_calls && !toolCallsReceived) {
          toolCallsReceived = true;
          _logger.debug('ğŸ”§ [ChatService] Tool calls detected in stream');
        }

        const finishReason = chunk.choices[0]?.finish_reason;
        if (finishReason) {
          _logger.debug('ğŸ [ChatService] Stream finished with reason:', finishReason);
          _logger.debug('ğŸ“Š [ChatService] Stream summary:', {
            totalChunks: chunkCount,
            totalContentLength: totalContent.length,
            totalReasoningContentLength: totalReasoningContent.length,
            hadToolCalls: toolCallsReceived,
            duration: Date.now() - startTime + 'ms',
          });
        }

        yield {
          content: delta.content || undefined,
          reasoningContent: reasoningChunk,
          toolCalls: delta.tool_calls,
          finishReason: finishReason || undefined,
        };
      }

      _logger.debug('âœ… [ChatService] Stream completed successfully');
    } catch (error) {
      const requestDuration = Date.now() - startTime;
      _logger.error(
        'âŒ [ChatService] Stream request failed after',
        requestDuration,
        'ms'
      );
      _logger.error('âŒ [ChatService] Stream error details:', error);
      throw error;
    }
  }

  getConfig(): ChatConfig {
    return { ...this.config };
  }

  updateConfig(newConfig: Partial<ChatConfig>): void {
    _logger.debug('ğŸ”„ [ChatService] Updating configuration');
    _logger.debug('ğŸ”„ [ChatService] New config:', {
      model: newConfig.model,
      baseUrl: newConfig.baseUrl,
      temperature: newConfig.temperature,
      maxContextTokens: newConfig.maxContextTokens,
      timeout: newConfig.timeout,
      hasApiKey: !!newConfig.apiKey,
    });

    const oldConfig = { ...this.config };
    this.config = { ...this.config, ...newConfig };

    // å¦‚æœ baseUrl å˜åŒ–ï¼Œé‡ç½®æ£€æµ‹åˆ°çš„å­—æ®µåï¼ˆä¸åŒä»£ç†å¯èƒ½ä½¿ç”¨ä¸åŒå­—æ®µåï¼‰
    if (oldConfig.baseUrl !== this.config.baseUrl) {
      this.detectedReasoningFieldName = null;
      _logger.debug('ğŸ”„ [ChatService] Reset detectedReasoningFieldName due to baseUrl change');
    }

    this.client = new OpenAI({
      apiKey: this.config.apiKey,
      baseURL: this.config.baseUrl, // OpenAI SDK ä½¿ç”¨ baseURL
      timeout: this.config.timeout ?? 180000, // 180ç§’è¶…æ—¶ï¼ˆé•¿ä¸Šä¸‹æ–‡åœºæ™¯éœ€è¦æ›´é•¿æ—¶é—´ï¼‰
      maxRetries: 2, // 2æ¬¡é‡è¯•ï¼Œå¹³è¡¡ç¨³å®šæ€§å’Œå“åº”é€Ÿåº¦
    });

    _logger.debug('âœ… [ChatService] Configuration updated successfully');
    _logger.debug('ğŸ“Š [ChatService] Config changes:', {
      modelChanged: oldConfig.model !== this.config.model,
      baseUrlChanged: oldConfig.baseUrl !== this.config.baseUrl,
      temperatureChanged: oldConfig.temperature !== this.config.temperature,
      maxContextTokensChanged:
        oldConfig.maxContextTokens !== this.config.maxContextTokens,
      timeoutChanged: oldConfig.timeout !== this.config.timeout,
      apiKeyChanged: oldConfig.apiKey !== this.config.apiKey,
    });
  }
}

/**
 * å‘åå…¼å®¹å¯¼å‡º
 */
export { OpenAIChatService as ChatService };
