import OpenAI from 'openai';
import type {
  ChatCompletionMessageParam,
  ChatCompletionMessageToolCall,
  ChatCompletionTool,
} from 'openai/resources/chat';
import type {
  ChatConfig,
  ChatResponse,
  IChatService,
  Message,
  StreamChunk,
} from './ChatServiceInterface.js';

export class OpenAIChatService implements IChatService {
  private client: OpenAI;

  constructor(private config: ChatConfig) {
    console.log('ğŸš€ [ChatService] Initializing ChatService');
    console.log('âš™ï¸ [ChatService] Config:', {
      model: config.model,
      baseUrl: config.baseUrl,
      temperature: config.temperature,
      maxTokens: config.maxTokens,
      timeout: config.timeout,
      hasApiKey: !!config.apiKey,
    });

    if (!config.baseUrl) {
      console.error('âŒ [ChatService] baseUrl is required in ChatConfig');
      throw new Error('baseUrl is required in ChatConfig');
    }
    if (!config.apiKey) {
      console.error('âŒ [ChatService] apiKey is required in ChatConfig');
      throw new Error('apiKey is required in ChatConfig');
    }
    if (!config.model) {
      console.error('âŒ [ChatService] model is required in ChatConfig');
      throw new Error('model is required in ChatConfig');
    }

    this.client = new OpenAI({
      apiKey: config.apiKey,
      baseURL: config.baseUrl, // OpenAI SDK ä½¿ç”¨ baseURL
      timeout: config.timeout ?? 90000, // 90ç§’è¶…æ—¶ï¼Œå‚è€ƒä¸»æµ CLI agent æ ‡å‡†
      maxRetries: 3,
    });

    console.log('âœ… [ChatService] ChatService initialized successfully');
  }

  async chat(
    messages: Message[],
    tools?: Array<{
      name: string;
      description: string;
      parameters: any;
    }>
  ): Promise<ChatResponse> {
    const startTime = Date.now();
    console.log('ğŸš€ [ChatService] Starting chat request');
    console.log('ğŸ“ [ChatService] Messages count:', messages.length);
    console.log(
      'ğŸ“ [ChatService] Messages preview:',
      messages.map((m) => ({ role: m.role, contentLength: m.content.length }))
    );

    const openaiMessages: ChatCompletionMessageParam[] = messages.map((msg) => {
      if (msg.role === 'tool') {
        return {
          role: 'tool',
          content: msg.content,
          tool_call_id: msg.tool_call_id!,
        };
      }
      if (msg.role === 'assistant' && msg.tool_calls) {
        return {
          role: 'assistant',
          content: msg.content || null,
          tool_calls: msg.tool_calls,
        };
      }
      return {
        role: msg.role as 'user' | 'assistant' | 'system',
        content: msg.content,
      };
    });

    const openaiTools: ChatCompletionTool[] | undefined = tools?.map((tool) => ({
      type: 'function' as const,
      function: {
        name: tool.name,
        description: tool.description,
        parameters: tool.parameters,
      },
    }));

    console.log('ğŸ”§ [ChatService] Tools count:', openaiTools?.length || 0);
    if (openaiTools && openaiTools.length > 0) {
      console.log(
        'ğŸ”§ [ChatService] Available tools:',
        openaiTools.map((t) => (t.type === 'function' ? t.function.name : 'unknown'))
      );
    }

    const requestParams = {
      model: this.config.model,
      messages: openaiMessages,
      tools: openaiTools,
      tool_choice:
        openaiTools && openaiTools.length > 0 ? ('auto' as const) : undefined,
      max_tokens: this.config.maxTokens ?? 32000,
      temperature: this.config.temperature ?? 0.0,
    };

    console.log('ğŸ“¤ [ChatService] Request params:', {
      model: requestParams.model,
      messagesCount: requestParams.messages.length,
      toolsCount: requestParams.tools?.length || 0,
      tool_choice: requestParams.tool_choice,
      max_tokens: requestParams.max_tokens,
      temperature: requestParams.temperature,
    });

    try {
      const completion = await this.client.chat.completions.create(requestParams);
      const requestDuration = Date.now() - startTime;

      console.log('ğŸ“¥ [ChatService] Response received in', requestDuration, 'ms');

      // âœ… éªŒè¯å“åº”æ ¼å¼
      if (!completion) {
        console.error('âŒ [ChatService] API returned null/undefined response');
        throw new Error('API returned null/undefined response');
      }

      if (!completion.choices || !Array.isArray(completion.choices)) {
        console.error(
          'âŒ [ChatService] Invalid API response format - missing choices array'
        );
        console.error(
          'âŒ [ChatService] Response object:',
          JSON.stringify(completion, null, 2)
        );
        throw new Error(
          `Invalid API response: missing choices array. Response: ${JSON.stringify(completion)}`
        );
      }

      if (completion.choices.length === 0) {
        console.error('âŒ [ChatService] API returned empty choices array');
        throw new Error('API returned empty choices array');
      }

      console.log('ğŸ“Š [ChatService] Response usage:', completion.usage);
      console.log(
        'ğŸ“Š [ChatService] Response choices count:',
        completion.choices.length
      );

      const choice = completion.choices[0];
      if (!choice) {
        console.error('âŒ [ChatService] No completion choice returned');
        throw new Error('No completion choice returned');
      }

      console.log('ğŸ“ [ChatService] Response choice:', {
        finishReason: choice.finish_reason,
        contentLength: choice.message.content?.length || 0,
        hasToolCalls: !!choice.message.tool_calls,
        toolCallsCount: choice.message.tool_calls?.length || 0,
      });

      if (choice.message.tool_calls) {
        console.log(
          'ğŸ”§ [ChatService] Tool calls:',
          choice.message.tool_calls.map((tc) => ({
            id: tc.id,
            type: tc.type,
            functionName: tc.type === 'function' ? tc.function?.name : 'unknown',
            functionArgsLength:
              tc.type === 'function' ? tc.function?.arguments?.length || 0 : 0,
          }))
        );
      }

      const toolCalls = choice.message.tool_calls?.filter(
        (tc): tc is ChatCompletionMessageToolCall => tc.type === 'function'
      );

      const response = {
        content: choice.message.content || '',
        toolCalls: toolCalls,
        usage: {
          promptTokens: completion.usage?.prompt_tokens || 0,
          completionTokens: completion.usage?.completion_tokens || 0,
          totalTokens: completion.usage?.total_tokens || 0,
        },
      };

      console.log('âœ… [ChatService] Chat completed successfully');
      console.log('ğŸ“Š [ChatService] Final response:', {
        contentLength: response.content.length,
        toolCallsCount: response.toolCalls?.length || 0,
        usage: response.usage,
      });

      return response;
    } catch (error) {
      const requestDuration = Date.now() - startTime;
      console.error(
        'âŒ [ChatService] Chat request failed after',
        requestDuration,
        'ms'
      );
      console.error('âŒ [ChatService] Error details:', error);
      throw error;
    }
  }

  async *streamChat(
    messages: Message[],
    tools?: Array<{
      name: string;
      description: string;
      // biome-ignore lint/suspicious/noExplicitAny: å·¥å…·å‚æ•°æ ¼å¼ä¸ç¡®å®š
      parameters: any;
    }>
  ): AsyncGenerator<StreamChunk, void, unknown> {
    const startTime = Date.now();
    console.log('ğŸš€ [ChatService] Starting chat stream request');
    console.log('ğŸ“ [ChatService] Messages count:', messages.length);
    console.log(
      'ğŸ“ [ChatService] Messages preview:',
      messages.map((m) => ({ role: m.role, contentLength: m.content.length }))
    );

    const openaiMessages: ChatCompletionMessageParam[] = messages.map((msg) => {
      if (msg.role === 'tool') {
        return {
          role: 'tool',
          content: msg.content,
          tool_call_id: msg.tool_call_id!,
        };
      }
      if (msg.role === 'assistant' && msg.tool_calls) {
        return {
          role: 'assistant',
          content: msg.content || null,
          tool_calls: msg.tool_calls,
        };
      }
      return {
        role: msg.role as 'user' | 'assistant' | 'system',
        content: msg.content,
      };
    });

    const openaiTools: ChatCompletionTool[] | undefined = tools?.map((tool) => ({
      type: 'function' as const,
      function: {
        name: tool.name,
        description: tool.description,
        parameters: tool.parameters,
      },
    }));

    console.log('ğŸ”§ [ChatService] Stream tools count:', openaiTools?.length || 0);
    if (openaiTools && openaiTools.length > 0) {
      console.log(
        'ğŸ”§ [ChatService] Stream available tools:',
        openaiTools.map((t) => (t.type === 'function' ? t.function.name : 'unknown'))
      );
    }

    const requestParams = {
      model: this.config.model,
      messages: openaiMessages,
      tools: openaiTools,
      tool_choice:
        openaiTools && openaiTools.length > 0 ? ('auto' as const) : ('none' as const),
      max_tokens: this.config.maxTokens ?? 32000,
      temperature: this.config.temperature ?? 0.0,
      stream: true as const,
    };

    console.log('ğŸ“¤ [ChatService] Stream request params:', {
      model: requestParams.model,
      messagesCount: requestParams.messages.length,
      toolsCount: requestParams.tools?.length || 0,
      tool_choice: requestParams.tool_choice,
      max_tokens: requestParams.max_tokens,
      temperature: requestParams.temperature,
      stream: requestParams.stream,
    });

    try {
      const stream = await this.client.chat.completions.create(requestParams);
      const requestDuration = Date.now() - startTime;
      console.log('ğŸ“¥ [ChatService] Stream started in', requestDuration, 'ms');

      let chunkCount = 0;
      let totalContent = '';
      let toolCallsReceived = false;

      for await (const chunk of stream) {
        chunkCount++;

        // âœ… éªŒè¯ chunk æ ¼å¼
        if (!chunk || !chunk.choices || !Array.isArray(chunk.choices)) {
          console.warn('âš ï¸ [ChatService] Invalid chunk format in stream', chunkCount);
          continue;
        }

        const delta = chunk.choices[0]?.delta;
        if (!delta) {
          console.log('âš ï¸ [ChatService] Empty delta in chunk', chunkCount);
          continue;
        }

        if (delta.content) {
          totalContent += delta.content;
        }

        if (delta.tool_calls && !toolCallsReceived) {
          toolCallsReceived = true;
          console.log('ğŸ”§ [ChatService] Tool calls detected in stream');
        }

        const finishReason = chunk.choices[0]?.finish_reason;
        if (finishReason) {
          console.log('ğŸ [ChatService] Stream finished with reason:', finishReason);
          console.log('ğŸ“Š [ChatService] Stream summary:', {
            totalChunks: chunkCount,
            totalContentLength: totalContent.length,
            hadToolCalls: toolCallsReceived,
            duration: Date.now() - startTime + 'ms',
          });
        }

        yield {
          content: delta.content || undefined,
          toolCalls: delta.tool_calls,
          finishReason: finishReason || undefined,
        };
      }

      console.log('âœ… [ChatService] Stream completed successfully');
    } catch (error) {
      const requestDuration = Date.now() - startTime;
      console.error(
        'âŒ [ChatService] Stream request failed after',
        requestDuration,
        'ms'
      );
      console.error('âŒ [ChatService] Stream error details:', error);
      throw error;
    }
  }

  getConfig(): ChatConfig {
    return { ...this.config };
  }

  updateConfig(newConfig: Partial<ChatConfig>): void {
    console.log('ğŸ”„ [ChatService] Updating configuration');
    console.log('ğŸ”„ [ChatService] New config:', {
      model: newConfig.model,
      baseUrl: newConfig.baseUrl,
      temperature: newConfig.temperature,
      maxTokens: newConfig.maxTokens,
      timeout: newConfig.timeout,
      hasApiKey: !!newConfig.apiKey,
    });

    const oldConfig = { ...this.config };
    this.config = { ...this.config, ...newConfig };

    this.client = new OpenAI({
      apiKey: this.config.apiKey,
      baseURL: this.config.baseUrl, // OpenAI SDK ä½¿ç”¨ baseURL
      timeout: this.config.timeout ?? 90000, // 90ç§’è¶…æ—¶ï¼Œå‚è€ƒä¸»æµ CLI agent æ ‡å‡†
      maxRetries: 2, // 2æ¬¡é‡è¯•ï¼Œå¹³è¡¡ç¨³å®šæ€§å’Œå“åº”é€Ÿåº¦
    });

    console.log('âœ… [ChatService] Configuration updated successfully');
    console.log('ğŸ“Š [ChatService] Config changes:', {
      modelChanged: oldConfig.model !== this.config.model,
      baseUrlChanged: oldConfig.baseUrl !== this.config.baseUrl,
      temperatureChanged: oldConfig.temperature !== this.config.temperature,
      maxTokensChanged: oldConfig.maxTokens !== this.config.maxTokens,
      timeoutChanged: oldConfig.timeout !== this.config.timeout,
      apiKeyChanged: oldConfig.apiKey !== this.config.apiKey,
    });
  }
}

/**
 * å‘åå…¼å®¹å¯¼å‡º
 */
export { OpenAIChatService as ChatService };

/**
 * é‡æ–°å¯¼å‡ºç±»å‹ï¼ˆå‘åå…¼å®¹ï¼‰
 */
export type {
  ChatConfig,
  ChatResponse,
  Message,
  StreamChunk,
} from './ChatServiceInterface.js';
