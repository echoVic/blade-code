/**
 * Azure OpenAI Chat Service
 *
 * ä½¿ç”¨ OpenAI SDK çš„ Azure é…ç½®å®ç° Azure OpenAI API çš„èŠå¤©æœåŠ¡
 *
 * ä¸»è¦å·®å¼‚ï¼ˆä¸æ ‡å‡† OpenAI API ç›¸æ¯”ï¼‰ï¼š
 * 1. ç«¯ç‚¹æ ¼å¼ä¸åŒï¼šhttps://{resource}.openai.azure.com/openai/deployments/{deployment}
 * 2. ä½¿ç”¨ api-key å¤´è€Œé Authorization å¤´
 * 3. éœ€è¦æŒ‡å®š api-version å‚æ•°
 * 4. model å‚æ•°æ˜¯éƒ¨ç½²åç§°è€Œéæ¨¡å‹ ID
 */

import { AzureOpenAI } from 'openai';
import type {
  ChatCompletionMessageParam,
  ChatCompletionMessageToolCall,
  ChatCompletionTool,
} from 'openai/resources/chat';
import { createLogger, LogCategory } from '../logging/Logger.js';
import type {
  ChatConfig,
  ChatResponse,
  IChatService,
  Message,
  StreamChunk,
} from './ChatServiceInterface.js';

const _logger = createLogger(LogCategory.CHAT);

/**
 * è¿‡æ»¤å­¤å„¿ tool æ¶ˆæ¯
 */
function filterOrphanToolMessages(messages: Message[]): Message[] {
  const availableToolCallIds = new Set<string>();
  for (const msg of messages) {
    if (msg.role === 'assistant' && msg.tool_calls) {
      for (const tc of msg.tool_calls) {
        availableToolCallIds.add(tc.id);
      }
    }
  }

  return messages.filter((msg) => {
    if (msg.role === 'tool') {
      if (!msg.tool_call_id) {
        return false;
      }
      return availableToolCallIds.has(msg.tool_call_id);
    }
    return true;
  });
}

export class AzureOpenAIChatService implements IChatService {
  private client: AzureOpenAI;
  private config: ChatConfig;

  constructor(config: ChatConfig) {
    this.config = config;

    _logger.debug('ğŸš€ [AzureOpenAIChatService] Initializing');
    _logger.debug('âš™ï¸ [AzureOpenAIChatService] Config:', {
      model: config.model,
      baseUrl: config.baseUrl,
      temperature: config.temperature,
      maxContextTokens: config.maxContextTokens,
      timeout: config.timeout,
      apiVersion: config.apiVersion,
      hasApiKey: !!config.apiKey,
    });

    if (!config.apiKey) {
      _logger.error('âŒ [AzureOpenAIChatService] apiKey is required');
      throw new Error('apiKey is required in ChatConfig');
    }
    if (!config.baseUrl) {
      _logger.error('âŒ [AzureOpenAIChatService] baseUrl is required');
      throw new Error('baseUrl is required in ChatConfig');
    }
    if (!config.model) {
      _logger.error('âŒ [AzureOpenAIChatService] model (deployment) is required');
      throw new Error('model (deployment) is required in ChatConfig');
    }

    // Azure OpenAI éœ€è¦é…ç½® endpoint å’Œ apiVersion
    // baseUrl åº”è¯¥æ˜¯ Azure OpenAI ç«¯ç‚¹ï¼Œå¦‚ https://{resource}.openai.azure.com
    this.client = new AzureOpenAI({
      apiKey: config.apiKey,
      endpoint: config.baseUrl,
      apiVersion: config.apiVersion || '2024-08-01-preview',
      timeout: config.timeout ?? 180000,
      maxRetries: 3,
    });

    _logger.debug('âœ… [AzureOpenAIChatService] Initialized successfully');
  }

  /**
   * å°†å†…éƒ¨ Message è½¬æ¢ä¸º OpenAI API æ ¼å¼
   */
  private convertToOpenAIMessages(messages: Message[]): ChatCompletionMessageParam[] {
    return messages.map((msg) => {
      if (msg.role === 'tool') {
        const toolContent =
          typeof msg.content === 'string'
            ? msg.content
            : msg.content
                .filter((p) => p.type === 'text')
                .map((p) => p.text)
                .join('\n');
        return {
          role: 'tool',
          content: toolContent,
          tool_call_id: msg.tool_call_id!,
        };
      }

      if (msg.role === 'assistant' && msg.tool_calls) {
        const assistantContent =
          typeof msg.content === 'string'
            ? msg.content
            : msg.content
                .filter((p) => p.type === 'text')
                .map((p) => p.text)
                .join('\n');

        return {
          role: 'assistant',
          content: assistantContent || null,
          tool_calls: msg.tool_calls,
        };
      }

      if (msg.role === 'user' && Array.isArray(msg.content)) {
        return {
          role: 'user',
          content: msg.content.map((part) => {
            if (part.type === 'text') {
              return { type: 'text' as const, text: part.text };
            }
            return {
              type: 'image_url' as const,
              image_url: { url: part.image_url.url },
            };
          }),
        };
      }

      return {
        role: msg.role as 'user' | 'assistant' | 'system',
        content:
          typeof msg.content === 'string'
            ? msg.content
            : msg.content
                .filter((p) => p.type === 'text')
                .map((p) => p.text)
                .join('\n'),
      };
    });
  }

  /**
   * å°†å·¥å…·å®šä¹‰è½¬æ¢ä¸º OpenAI API æ ¼å¼
   */
  private convertToOpenAITools(
    tools?: Array<{ name: string; description: string; parameters: unknown }>
  ): ChatCompletionTool[] | undefined {
    return tools?.map((tool) => ({
      type: 'function' as const,
      function: {
        name: tool.name,
        description: tool.description,
        parameters: tool.parameters as Record<string, unknown>,
      },
    }));
  }

  async chat(
    messages: Message[],
    tools?: Array<{
      name: string;
      description: string;
      parameters: unknown;
    }>,
    signal?: AbortSignal
  ): Promise<ChatResponse> {
    const startTime = Date.now();
    _logger.debug('ğŸš€ [AzureOpenAIChatService] Starting chat request');
    _logger.debug('ğŸ“ [AzureOpenAIChatService] Messages count:', messages.length);

    const filteredMessages = filterOrphanToolMessages(messages);
    if (filteredMessages.length < messages.length) {
      _logger.debug(
        `ğŸ”§ [AzureOpenAIChatService] è¿‡æ»¤æ‰ ${messages.length - filteredMessages.length} æ¡å­¤å„¿ tool æ¶ˆæ¯`
      );
    }

    const openaiMessages = this.convertToOpenAIMessages(filteredMessages);
    const openaiTools = this.convertToOpenAITools(tools);

    _logger.debug('ğŸ”§ [AzureOpenAIChatService] Tools count:', openaiTools?.length || 0);

    const requestParams = {
      model: this.config.model, // Azure ä¸­è¿™æ˜¯éƒ¨ç½²åç§°
      messages: openaiMessages,
      tools: openaiTools,
      tool_choice:
        openaiTools && openaiTools.length > 0 ? ('auto' as const) : undefined,
      max_tokens: this.config.maxOutputTokens ?? 32768,
      temperature: this.config.temperature ?? 0.0,
    };

    _logger.debug('ğŸ“¤ [AzureOpenAIChatService] Request params:', {
      model: requestParams.model,
      messagesCount: requestParams.messages.length,
      toolsCount: requestParams.tools?.length || 0,
      max_tokens: requestParams.max_tokens,
      temperature: requestParams.temperature,
    });

    try {
      const completion = await this.client.chat.completions.create(requestParams, {
        signal,
      });
      const requestDuration = Date.now() - startTime;

      _logger.debug(
        'ğŸ“¥ [AzureOpenAIChatService] Response received in',
        requestDuration,
        'ms'
      );

      if (!completion || !completion.choices || completion.choices.length === 0) {
        throw new Error('Invalid API response: missing choices');
      }

      const choice = completion.choices[0];
      if (!choice) {
        throw new Error('No completion choice returned');
      }

      const toolCalls = choice.message.tool_calls?.filter(
        (tc): tc is ChatCompletionMessageToolCall => tc.type === 'function'
      );

      const response = {
        content: choice.message.content || '',
        toolCalls: toolCalls,
        usage: {
          promptTokens: completion.usage?.prompt_tokens || 0,
          completionTokens: completion.usage?.completion_tokens || 0,
          totalTokens: completion.usage?.total_tokens || 0,
        },
      };

      _logger.debug('âœ… [AzureOpenAIChatService] Chat completed successfully');
      _logger.debug('ğŸ“Š [AzureOpenAIChatService] Final response:', {
        contentLength: response.content.length,
        toolCallsCount: response.toolCalls?.length || 0,
        usage: response.usage,
      });

      return response;
    } catch (error) {
      const requestDuration = Date.now() - startTime;
      _logger.error(
        'âŒ [AzureOpenAIChatService] Chat request failed after',
        requestDuration,
        'ms'
      );
      _logger.error('âŒ [AzureOpenAIChatService] Error details:', error);
      throw error;
    }
  }

  async *streamChat(
    messages: Message[],
    tools?: Array<{
      name: string;
      description: string;
      parameters: unknown;
    }>,
    signal?: AbortSignal
  ): AsyncGenerator<StreamChunk, void, unknown> {
    const startTime = Date.now();
    _logger.debug('ğŸš€ [AzureOpenAIChatService] Starting stream request');
    _logger.debug('ğŸ“ [AzureOpenAIChatService] Messages count:', messages.length);

    const filteredMessages = filterOrphanToolMessages(messages);
    if (filteredMessages.length < messages.length) {
      _logger.debug(
        `ğŸ”§ [AzureOpenAIChatService] è¿‡æ»¤æ‰ ${messages.length - filteredMessages.length} æ¡å­¤å„¿ tool æ¶ˆæ¯`
      );
    }

    const openaiMessages = this.convertToOpenAIMessages(filteredMessages);
    const openaiTools = this.convertToOpenAITools(tools);

    _logger.debug(
      'ğŸ”§ [AzureOpenAIChatService] Stream tools count:',
      openaiTools?.length || 0
    );

    const requestParams = {
      model: this.config.model,
      messages: openaiMessages,
      tools: openaiTools,
      tool_choice:
        openaiTools && openaiTools.length > 0 ? ('auto' as const) : undefined,
      max_tokens: this.config.maxOutputTokens ?? 32768,
      temperature: this.config.temperature ?? 0.0,
      stream: true as const,
    };

    _logger.debug('ğŸ“¤ [AzureOpenAIChatService] Stream request params:', {
      model: requestParams.model,
      messagesCount: requestParams.messages.length,
      toolsCount: requestParams.tools?.length || 0,
      max_tokens: requestParams.max_tokens,
      temperature: requestParams.temperature,
    });

    try {
      const stream = await this.client.chat.completions.create(requestParams, {
        signal,
      });
      const requestDuration = Date.now() - startTime;
      _logger.debug(
        'ğŸ“¥ [AzureOpenAIChatService] Stream started in',
        requestDuration,
        'ms'
      );

      let chunkCount = 0;
      let totalContent = '';
      let toolCallsReceived = false;

      for await (const chunk of stream) {
        chunkCount++;

        if (!chunk || !chunk.choices || !Array.isArray(chunk.choices)) {
          _logger.warn(
            'âš ï¸ [AzureOpenAIChatService] Invalid chunk format in stream',
            chunkCount
          );
          continue;
        }

        const delta = chunk.choices[0]?.delta;
        if (!delta) {
          _logger.warn('âš ï¸ [AzureOpenAIChatService] Empty delta in chunk', chunkCount);
          continue;
        }

        if (delta.content) {
          totalContent += delta.content;
        }

        if (delta.tool_calls && !toolCallsReceived) {
          toolCallsReceived = true;
          _logger.debug('ğŸ”§ [AzureOpenAIChatService] Tool calls detected in stream');
        }

        const finishReason = chunk.choices[0]?.finish_reason;
        if (finishReason) {
          _logger.debug(
            'ğŸ [AzureOpenAIChatService] Stream finished with reason:',
            finishReason
          );
          _logger.debug('ğŸ“Š [AzureOpenAIChatService] Stream summary:', {
            totalChunks: chunkCount,
            totalContentLength: totalContent.length,
            hadToolCalls: toolCallsReceived,
            duration: Date.now() - startTime + 'ms',
          });
        }

        yield {
          content: delta.content || undefined,
          toolCalls: delta.tool_calls,
          finishReason: finishReason || undefined,
        };
      }

      _logger.debug('âœ… [AzureOpenAIChatService] Stream completed successfully');
    } catch (error) {
      const requestDuration = Date.now() - startTime;
      _logger.error(
        'âŒ [AzureOpenAIChatService] Stream request failed after',
        requestDuration,
        'ms'
      );
      _logger.error('âŒ [AzureOpenAIChatService] Stream error details:', error);
      throw error;
    }
  }

  getConfig(): ChatConfig {
    return { ...this.config };
  }

  updateConfig(newConfig: Partial<ChatConfig>): void {
    _logger.debug('ğŸ”„ [AzureOpenAIChatService] Updating configuration');

    this.config = { ...this.config, ...newConfig };

    this.client = new AzureOpenAI({
      apiKey: this.config.apiKey,
      endpoint: this.config.baseUrl,
      apiVersion: this.config.apiVersion || '2024-08-01-preview',
      timeout: this.config.timeout ?? 180000,
      maxRetries: 3,
    });

    _logger.debug('âœ… [AzureOpenAIChatService] Configuration updated successfully');
  }
}
