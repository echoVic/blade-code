/**
 * GPT OpenAI Platform ChatService
 * å­—èŠ‚è·³åŠ¨å†…éƒ¨ GPT å¹³å°ä¸“ç”¨çš„ ChatService å®ç°
 *
 * ä¸»è¦ç‰¹ç‚¹ï¼š
 * 1. æ¯æ¬¡è¯·æ±‚è‡ªåŠ¨ç”Ÿæˆ x-tt-logid header
 * 2. æ”¯æŒå¯é€‰çš„ apiVersion é…ç½®
 */

import { randomUUID } from 'node:crypto';
import OpenAI from 'openai';
import type {
  ChatCompletionMessageParam,
  ChatCompletionMessageToolCall,
  ChatCompletionTool,
} from 'openai/resources/chat';
import { createLogger, LogCategory } from '../logging/Logger.js';
import type {
  ChatConfig,
  ChatResponse,
  IChatService,
  Message,
  StreamChunk,
} from './ChatServiceInterface.js';

const _logger = createLogger(LogCategory.CHAT);

/**
 * ç”Ÿæˆ x-tt-logid
 * æ ¼å¼ï¼š32ä½ hex + "01" åç¼€
 */
function generateLogId(): string {
  return randomUUID().replace(/-/g, '') + '01';
}

/**
 * æ£€æŸ¥æ˜¯å¦æ˜¯ Gemini æ¨¡å‹
 */
function isGeminiModel(model: string): boolean {
  return model.toLowerCase().includes('gemini');
}

/**
 * æ¸…ç†å·¥å…·å‚æ•°ä¸­ Gemini API ä¸æ”¯æŒçš„å­—æ®µ
 * Gemini ä¸æ”¯æŒ: $schema, additionalProperties
 */
// biome-ignore lint/suspicious/noExplicitAny: éœ€è¦å¤„ç†ä»»æ„ JSON Schema ç»“æ„
function cleanToolParametersForGemini(params: any): any {
  if (!params || typeof params !== 'object') {
    return params;
  }

  if (Array.isArray(params)) {
    return params.map(cleanToolParametersForGemini);
  }

  const cleaned: Record<string, unknown> = {};
  for (const [key, value] of Object.entries(params)) {
    // è·³è¿‡ Gemini ä¸æ”¯æŒçš„å­—æ®µ
    if (key === '$schema' || key === 'additionalProperties') {
      continue;
    }
    cleaned[key] = cleanToolParametersForGemini(value);
  }
  return cleaned;
}

/**
 * ä¸º Gemini ä¿®å¤æ¶ˆæ¯ï¼Œç¡®ä¿ tool_call å’Œ tool_result ä¸€ä¸€å¯¹åº”
 *
 * Gemini API è¦æ±‚ï¼š
 * 1. æ¯ä¸ª tool_call å¿…é¡»æœ‰å¯¹åº”çš„ tool result
 * 2. æ¯ä¸ª tool result å¿…é¡»æœ‰å¯¹åº”çš„ tool_call
 *
 * ç­–ç•¥ï¼š
 * - å­¤å„¿ tool resultsï¼ˆæ²¡æœ‰å¯¹åº” tool_callï¼‰ï¼šåˆ é™¤
 * - å­¤å„¿ tool_callsï¼ˆæ²¡æœ‰å¯¹åº” resultï¼‰ï¼šæ·»åŠ å ä½ç¬¦ result
 * è¿™æ ·å¯ä»¥ä¿ç•™ä¸Šä¸‹æ–‡ï¼Œé¿å…æ­»å¾ªç¯
 */
function fixMessagesForGemini(messages: Message[]): Message[] {
  // 1. æ”¶é›†æ‰€æœ‰ tool_call IDs åŠå…¶ä½ç½®
  const toolCallMap = new Map<string, number>(); // id -> assistant message index
  for (let i = 0; i < messages.length; i++) {
    const msg = messages[i];
    if (msg.role === 'assistant' && msg.tool_calls) {
      for (const tc of msg.tool_calls) {
        toolCallMap.set(tc.id, i);
      }
    }
  }

  // 2. æ”¶é›†æ‰€æœ‰ tool result çš„ tool_call_ids
  const toolResultIds = new Set<string>();
  for (const msg of messages) {
    if (msg.role === 'tool' && msg.tool_call_id) {
      toolResultIds.add(msg.tool_call_id);
    }
  }

  // 3. æ‰¾åˆ°éœ€è¦è¡¥å……çš„ tool_call IDsï¼ˆæœ‰ tool_call ä½†æ²¡æœ‰ resultï¼‰
  const missingResultIds: string[] = [];
  for (const id of toolCallMap.keys()) {
    if (!toolResultIds.has(id)) {
      missingResultIds.push(id);
    }
  }

  // 4. æ„å»ºä¿®å¤åçš„æ¶ˆæ¯åˆ—è¡¨
  const fixed: Message[] = [];
  for (const msg of messages) {
    if (msg.role === 'tool') {
      // åªä¿ç•™æœ‰å¯¹åº” tool_call çš„ tool results
      if (msg.tool_call_id && toolCallMap.has(msg.tool_call_id)) {
        fixed.push(msg);
      }
      // å­¤å„¿ tool results è¢«ä¸¢å¼ƒ
    } else {
      fixed.push(msg);

      // å¦‚æœæ˜¯ assistant æ¶ˆæ¯ä¸”æœ‰ tool_callsï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦è¡¥å…… results
      if (msg.role === 'assistant' && msg.tool_calls) {
        for (const tc of msg.tool_calls) {
          if (missingResultIds.includes(tc.id)) {
            // æ·»åŠ å ä½ç¬¦ tool result
            fixed.push({
              role: 'tool',
              content: '[Result lost due to context compression]',
              tool_call_id: tc.id,
            });
          }
        }
      }
    }
  }

  if (missingResultIds.length > 0) {
    _logger.debug(
      `ğŸ”§ [GptOpenaiPlatformChatService] Gemini ä¿®å¤ï¼šè¡¥å…… ${missingResultIds.length} ä¸ªç¼ºå¤±çš„ tool results`
    );
  }

  return fixed;
}

export class GptOpenaiPlatformChatService implements IChatService {
  private config: ChatConfig;

  constructor(config: ChatConfig) {
    _logger.debug('ğŸš€ [GptOpenaiPlatformChatService] Initializing');
    _logger.debug('âš™ï¸ [GptOpenaiPlatformChatService] Config:', {
      model: config.model,
      baseUrl: config.baseUrl,
      apiVersion: config.apiVersion,
      temperature: config.temperature,
      maxContextTokens: config.maxContextTokens,
      timeout: config.timeout,
      hasApiKey: !!config.apiKey,
    });

    if (!config.baseUrl) {
      _logger.error('âŒ [GptOpenaiPlatformChatService] baseUrl is required');
      throw new Error('baseUrl is required in ChatConfig');
    }
    if (!config.apiKey) {
      _logger.error('âŒ [GptOpenaiPlatformChatService] apiKey is required');
      throw new Error('apiKey is required in ChatConfig');
    }
    if (!config.model) {
      _logger.error('âŒ [GptOpenaiPlatformChatService] model is required');
      throw new Error('model is required in ChatConfig');
    }

    this.config = config;
    _logger.debug('âœ… [GptOpenaiPlatformChatService] Initialized successfully');
  }

  /**
   * åˆ›å»ºä¸€ä¸ªæ–°çš„ OpenAI å®¢æˆ·ç«¯ï¼ˆæ¯æ¬¡è¯·æ±‚éƒ½åˆ›å»ºï¼Œä»¥ç”Ÿæˆæ–°çš„ logidï¼‰
   */
  private createClient(): OpenAI {
    const logId = generateLogId();
    _logger.debug('ğŸ”‘ [GptOpenaiPlatformChatService] Generated logid:', logId);

    const defaultQuery: Record<string, string> = {};
    const defaultHeaders: Record<string, string> = {
      'api-key': this.config.apiKey,
      'x-tt-logid': logId,
    };

    // å¦‚æœé…ç½®äº† apiVersionï¼Œæ·»åŠ åˆ° query å‚æ•°ä¸­
    if (this.config.apiVersion) {
      defaultQuery['api-version'] = this.config.apiVersion;
    }

    return new OpenAI({
      apiKey: this.config.apiKey,
      baseURL: this.config.baseUrl,
      timeout: this.config.timeout ?? 180000,
      maxRetries: 3,
      defaultQuery,
      defaultHeaders,
    });
  }

  async chat(
    messages: Message[],
    tools?: Array<{
      name: string;
      description: string;
      // biome-ignore lint/suspicious/noExplicitAny: å·¥å…·å‚æ•°æ ¼å¼ä¸ç¡®å®š
      parameters: any;
    }>,
    signal?: AbortSignal
  ): Promise<ChatResponse> {
    const startTime = Date.now();
    _logger.debug('ğŸš€ [GptOpenaiPlatformChatService] Starting chat request');
    _logger.debug('ğŸ“ [GptOpenaiPlatformChatService] Messages count:', messages.length);

    const client = this.createClient();
    const isGemini = isGeminiModel(this.config.model);

    // Gemini éœ€è¦ä¿®å¤æ¶ˆæ¯ç¡®ä¿ tool_call/result é…å¯¹ï¼ˆè¡¥å……ç¼ºå¤±çš„ results è€Œéåˆ é™¤ï¼‰
    const filteredMessages = isGemini ? fixMessagesForGemini(messages) : messages;

    const openaiMessages: ChatCompletionMessageParam[] = filteredMessages.map((msg) => {
      if (msg.role === 'tool') {
        // tool æ¶ˆæ¯çš„ content éœ€è¦æ˜¯å­—ç¬¦ä¸²
        const toolContent = typeof msg.content === 'string'
          ? msg.content
          : msg.content.filter((p) => p.type === 'text').map((p) => (p as { text: string }).text).join('\n');
        return {
          role: 'tool',
          content: toolContent,
          tool_call_id: msg.tool_call_id!,
        };
      }
      if (msg.role === 'assistant' && msg.tool_calls) {
        // assistant æ¶ˆæ¯çš„ content éœ€è¦æ˜¯å­—ç¬¦ä¸²æˆ– null
        const assistantContent = typeof msg.content === 'string'
          ? msg.content
          : msg.content?.filter((p) => p.type === 'text').map((p) => (p as { text: string }).text).join('\n');
        return {
          role: 'assistant',
          content: assistantContent || null,
          tool_calls: msg.tool_calls,
        };
      }
      // å¤„ç† user/system/assistant æ¶ˆæ¯
      // user æ¶ˆæ¯å¯èƒ½åŒ…å«å¤šæ¨¡æ€å†…å®¹ï¼ˆæ–‡æœ¬ + å›¾ç‰‡ï¼‰
      if (msg.role === 'user' && Array.isArray(msg.content)) {
        // å¤šæ¨¡æ€ user æ¶ˆæ¯ï¼šè½¬æ¢ä¸º OpenAI Vision API æ ¼å¼
        return {
          role: 'user',
          content: msg.content.map((part) => {
            if (part.type === 'text') {
              return { type: 'text' as const, text: part.text };
            }
            // image_url ç±»å‹
            return {
              type: 'image_url' as const,
              image_url: { url: part.image_url.url },
            };
          }),
        };
      }
      // æ™®é€šæ–‡æœ¬æ¶ˆæ¯
      return {
        role: msg.role as 'user' | 'assistant' | 'system',
        content: typeof msg.content === 'string'
          ? msg.content
          : msg.content.filter((p) => p.type === 'text').map((p) => (p as { text: string }).text).join('\n'),
      };
    });

    const openaiTools: ChatCompletionTool[] | undefined = tools?.map((tool) => ({
      type: 'function' as const,
      function: {
        name: tool.name,
        description: tool.description,
        parameters: isGemini
          ? cleanToolParametersForGemini(tool.parameters)
          : tool.parameters,
      },
    }));

    _logger.debug(
      'ğŸ”§ [GptOpenaiPlatformChatService] Tools count:',
      openaiTools?.length || 0
    );

    const requestParams = {
      model: this.config.model,
      messages: openaiMessages,
      tools: openaiTools,
      tool_choice:
        openaiTools && openaiTools.length > 0 ? ('auto' as const) : undefined,
      max_tokens: this.config.maxOutputTokens ?? 32768,
      temperature: this.config.temperature ?? 0.0,
    };

    _logger.debug('ğŸ“¤ [GptOpenaiPlatformChatService] Request params:', {
      model: requestParams.model,
      messagesCount: requestParams.messages.length,
      toolsCount: requestParams.tools?.length || 0,
      tool_choice: requestParams.tool_choice,
      max_tokens: requestParams.max_tokens,
      temperature: requestParams.temperature,
    });

    try {
      const completion = await client.chat.completions.create(requestParams, {
        signal,
      });
      const requestDuration = Date.now() - startTime;

      _logger.debug(
        'ğŸ“¥ [GptOpenaiPlatformChatService] Response received in',
        requestDuration,
        'ms'
      );

      // éªŒè¯å“åº”æ ¼å¼
      if (!completion) {
        _logger.error(
          'âŒ [GptOpenaiPlatformChatService] API returned null/undefined response'
        );
        throw new Error('API returned null/undefined response');
      }

      if (!completion.choices || !Array.isArray(completion.choices)) {
        _logger.error(
          'âŒ [GptOpenaiPlatformChatService] Invalid API response format - missing choices array'
        );
        throw new Error(
          `Invalid API response: missing choices array. Response: ${JSON.stringify(completion)}`
        );
      }

      if (completion.choices.length === 0) {
        _logger.error(
          'âŒ [GptOpenaiPlatformChatService] API returned empty choices array'
        );
        throw new Error('API returned empty choices array');
      }

      const choice = completion.choices[0];
      if (!choice) {
        _logger.error(
          'âŒ [GptOpenaiPlatformChatService] No completion choice returned'
        );
        throw new Error('No completion choice returned');
      }

      _logger.debug('ğŸ“ [GptOpenaiPlatformChatService] Response choice:', {
        finishReason: choice.finish_reason,
        contentLength: choice.message.content?.length || 0,
        hasToolCalls: !!choice.message.tool_calls,
        toolCallsCount: choice.message.tool_calls?.length || 0,
      });

      const toolCalls = choice.message.tool_calls?.filter(
        (tc): tc is ChatCompletionMessageToolCall => tc.type === 'function'
      );

      // æå– reasoning_contentï¼ˆå¦‚æœæœ‰ï¼‰
      const extendedMessage = choice.message as typeof choice.message & {
        reasoning_content?: string;
      };
      const reasoningContent = extendedMessage.reasoning_content || undefined;

      // æå– reasoning_tokens
      const extendedUsage = completion.usage as typeof completion.usage & {
        reasoning_tokens?: number;
      };

      const response = {
        content: choice.message.content || '',
        reasoningContent,
        toolCalls: toolCalls,
        usage: {
          promptTokens: completion.usage?.prompt_tokens || 0,
          completionTokens: completion.usage?.completion_tokens || 0,
          totalTokens: completion.usage?.total_tokens || 0,
          reasoningTokens: extendedUsage?.reasoning_tokens,
        },
      };

      _logger.debug('âœ… [GptOpenaiPlatformChatService] Chat completed successfully');
      return response;
    } catch (error) {
      const requestDuration = Date.now() - startTime;
      _logger.error(
        'âŒ [GptOpenaiPlatformChatService] Chat request failed after',
        requestDuration,
        'ms'
      );
      _logger.error('âŒ [GptOpenaiPlatformChatService] Error details:', error);
      throw error;
    }
  }

  async *streamChat(
    messages: Message[],
    tools?: Array<{
      name: string;
      description: string;
      // biome-ignore lint/suspicious/noExplicitAny: å·¥å…·å‚æ•°æ ¼å¼ä¸ç¡®å®š
      parameters: any;
    }>,
    signal?: AbortSignal
  ): AsyncGenerator<StreamChunk, void, unknown> {
    const startTime = Date.now();
    _logger.debug('ğŸš€ [GptOpenaiPlatformChatService] Starting stream request');
    _logger.debug('ğŸ“ [GptOpenaiPlatformChatService] Messages count:', messages.length);

    const client = this.createClient();
    const isGemini = isGeminiModel(this.config.model);

    // Gemini éœ€è¦ä¿®å¤æ¶ˆæ¯ç¡®ä¿ tool_call/result é…å¯¹ï¼ˆè¡¥å……ç¼ºå¤±çš„ results è€Œéåˆ é™¤ï¼‰
    const filteredMessages = isGemini ? fixMessagesForGemini(messages) : messages;

    const openaiMessages: ChatCompletionMessageParam[] = filteredMessages.map((msg) => {
      if (msg.role === 'tool') {
        // tool æ¶ˆæ¯çš„ content éœ€è¦æ˜¯å­—ç¬¦ä¸²
        const toolContent = typeof msg.content === 'string'
          ? msg.content
          : msg.content.filter((p) => p.type === 'text').map((p) => (p as { text: string }).text).join('\n');
        return {
          role: 'tool',
          content: toolContent,
          tool_call_id: msg.tool_call_id!,
        };
      }
      if (msg.role === 'assistant' && msg.tool_calls) {
        // assistant æ¶ˆæ¯çš„ content éœ€è¦æ˜¯å­—ç¬¦ä¸²æˆ– null
        const assistantContent = typeof msg.content === 'string'
          ? msg.content
          : msg.content?.filter((p) => p.type === 'text').map((p) => (p as { text: string }).text).join('\n');
        return {
          role: 'assistant',
          content: assistantContent || null,
          tool_calls: msg.tool_calls,
        };
      }
      // å¤„ç† user/system/assistant æ¶ˆæ¯
      // user æ¶ˆæ¯å¯èƒ½åŒ…å«å¤šæ¨¡æ€å†…å®¹ï¼ˆæ–‡æœ¬ + å›¾ç‰‡ï¼‰
      if (msg.role === 'user' && Array.isArray(msg.content)) {
        // å¤šæ¨¡æ€ user æ¶ˆæ¯ï¼šè½¬æ¢ä¸º OpenAI Vision API æ ¼å¼
        return {
          role: 'user',
          content: msg.content.map((part) => {
            if (part.type === 'text') {
              return { type: 'text' as const, text: part.text };
            }
            // image_url ç±»å‹
            return {
              type: 'image_url' as const,
              image_url: { url: part.image_url.url },
            };
          }),
        };
      }
      // æ™®é€šæ–‡æœ¬æ¶ˆæ¯
      return {
        role: msg.role as 'user' | 'assistant' | 'system',
        content: typeof msg.content === 'string'
          ? msg.content
          : msg.content.filter((p) => p.type === 'text').map((p) => (p as { text: string }).text).join('\n'),
      };
    });

    const openaiTools: ChatCompletionTool[] | undefined = tools?.map((tool) => ({
      type: 'function' as const,
      function: {
        name: tool.name,
        description: tool.description,
        parameters: isGemini
          ? cleanToolParametersForGemini(tool.parameters)
          : tool.parameters,
      },
    }));

    const requestParams = {
      model: this.config.model,
      messages: openaiMessages,
      tools: openaiTools,
      tool_choice:
        openaiTools && openaiTools.length > 0 ? ('auto' as const) : ('none' as const),
      max_tokens: this.config.maxOutputTokens ?? 32768,
      temperature: this.config.temperature ?? 0.0,
      stream: true as const,
    };

    _logger.debug('ğŸ“¤ [GptOpenaiPlatformChatService] Stream request params:', {
      model: requestParams.model,
      messagesCount: requestParams.messages.length,
      toolsCount: requestParams.tools?.length || 0,
    });

    try {
      const stream = await client.chat.completions.create(requestParams, {
        signal,
      });
      const requestDuration = Date.now() - startTime;
      _logger.debug(
        'ğŸ“¥ [GptOpenaiPlatformChatService] Stream started in',
        requestDuration,
        'ms'
      );

      let chunkCount = 0;
      let totalContent = '';
      let totalReasoningContent = '';
      let toolCallsReceived = false;

      for await (const chunk of stream) {
        chunkCount++;

        if (!chunk || !chunk.choices || !Array.isArray(chunk.choices)) {
          _logger.warn(
            'âš ï¸ [GptOpenaiPlatformChatService] Invalid chunk format in stream',
            chunkCount
          );
          continue;
        }

        const delta = chunk.choices[0]?.delta;
        if (!delta) {
          continue;
        }

        const extendedDelta = delta as typeof delta & {
          reasoning_content?: string;
        };

        if (delta.content) {
          totalContent += delta.content;
        }

        if (extendedDelta.reasoning_content) {
          totalReasoningContent += extendedDelta.reasoning_content;
        }

        if (delta.tool_calls && !toolCallsReceived) {
          toolCallsReceived = true;
          _logger.debug(
            'ğŸ”§ [GptOpenaiPlatformChatService] Tool calls detected in stream'
          );
        }

        const finishReason = chunk.choices[0]?.finish_reason;
        if (finishReason) {
          _logger.debug(
            'ğŸ [GptOpenaiPlatformChatService] Stream finished with reason:',
            finishReason
          );
          _logger.debug('ğŸ“Š [GptOpenaiPlatformChatService] Stream summary:', {
            totalChunks: chunkCount,
            totalContentLength: totalContent.length,
            totalReasoningContentLength: totalReasoningContent.length,
            hadToolCalls: toolCallsReceived,
            duration: Date.now() - startTime + 'ms',
          });
        }

        yield {
          content: delta.content || undefined,
          reasoningContent: extendedDelta.reasoning_content || undefined,
          toolCalls: delta.tool_calls,
          finishReason: finishReason || undefined,
        };
      }

      _logger.debug('âœ… [GptOpenaiPlatformChatService] Stream completed successfully');
    } catch (error) {
      const requestDuration = Date.now() - startTime;
      _logger.error(
        'âŒ [GptOpenaiPlatformChatService] Stream request failed after',
        requestDuration,
        'ms'
      );
      _logger.error('âŒ [GptOpenaiPlatformChatService] Stream error details:', error);
      throw error;
    }
  }

  getConfig(): ChatConfig {
    return { ...this.config };
  }

  updateConfig(newConfig: Partial<ChatConfig>): void {
    _logger.debug('ğŸ”„ [GptOpenaiPlatformChatService] Updating configuration');
    this.config = { ...this.config, ...newConfig };
    _logger.debug(
      'âœ… [GptOpenaiPlatformChatService] Configuration updated successfully'
    );
  }
}
