/**
 * Blade AI Types - å…±äº«ç±»å‹å®šä¹‰
 */

// åŸºç¡€é…ç½®ç±»å‹
export interface BladeConfig {
  // ğŸ” è®¤è¯é…ç½® (ä¸‰è¦ç´ )
  apiKey: string;
  baseUrl?: string;
  modelName?: string;
  searchApiKey?: string;

  // ğŸ¨ UIé…ç½®  
  theme?: 'GitHub' | 'dark' | 'light' | 'auto';
  hideTips?: boolean;
  hideBanner?: boolean;

  // ğŸ”’ å®‰å…¨é…ç½®
  sandbox?: 'docker' | 'none';
  
  // ğŸ› ï¸ å·¥å…·é…ç½®
  toolDiscoveryCommand?: string;
  toolCallCommand?: string;
  summarizeToolOutput?: Record<string, { tokenBudget?: number }>;

  // ğŸ”— MCPé…ç½®
  mcpServers?: Record<string, {
    command: string;
    args?: string[];
    env?: Record<string, string>;
  }>;

  // ğŸ“Š é¥æµ‹é…ç½®
  telemetry?: {
    enabled?: boolean;
    target?: 'local' | 'remote';
    otlpEndpoint?: string;
    logPrompts?: boolean;
  };

  // ğŸ“ˆ ä½¿ç”¨é…ç½®
  usageStatisticsEnabled?: boolean;
  maxSessionTurns?: number;

  // ğŸ è°ƒè¯•é…ç½®
  debug?: boolean;
}

// LLM ç›¸å…³ç±»å‹
export interface LLMMessage {
  role: 'system' | 'user' | 'assistant';
  content: string;
}

export interface LLMRequest {
  messages: LLMMessage[];
  model?: string;
  temperature?: number;
  maxTokens?: number;
  stream?: boolean;
}

export interface LLMResponse {
  content: string;
  usage?: {
    promptTokens: number;
    completionTokens: number;
    totalTokens: number;
  };
  model?: string;
}

// å·¥å…·è°ƒç”¨ç›¸å…³ç±»å‹
export interface ToolConfig {
  name: string;
  description: string;
  parameters?: any;
}

export interface ToolResult {
  success: boolean;
  data?: any;
  error?: string;
}

// ä¸Šä¸‹æ–‡ç®¡ç†ç±»å‹
export interface ContextConfig {
  maxMessages?: number;
  maxTokens?: number;
  compressionEnabled?: boolean;
}

// Agent é…ç½®ç±»å‹
export interface AgentConfig {
  llm: {
    provider: 'qwen' | 'volcengine';
    apiKey: string;
    model?: string;
    baseURL?: string;
  };
  tools?: string[];
  context?: ContextConfig;
}